"""
Context Engine - ç»Ÿä¸€çš„æ™ºèƒ½ä¸Šä¸‹æ–‡å¼•æ“

æ•´åˆåŠŸèƒ½ï¼š
1. æ‘˜è¦ç”Ÿæˆä¸å­˜å‚¨ï¼ˆæ¥è‡ª auto_summary.pyï¼‰
2. ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆæ¥è‡ª context_injector.pyï¼‰
3. è§¦å‘è¯æ£€æµ‹
4. å…³é”®è¯æ³¨å…¥
5. ä¼šè¯æ¢å¤

è®©ç¬¬äºŒå¤§è„‘è¶Šæ¥è¶Šèªæ˜ - æ ¸å¿ƒå¼•æ“
"""

import json
import re
import os
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, asdict
from enum import Enum

from .nexus_core import NexusCore
from .session_manager import SessionManagerPlugin
from ..core.plugin_system import NexusPlugin, PluginMetadata
from ..core.event_bus import EventTypes


class MemoryTier(Enum):
    """è®°å¿†å±‚çº§"""
    HOT = "hot"    # æœ€è¿‘æ´»è·ƒ
    WARM = "warm"   # æœ€è¿‘ä½¿ç”¨
    COLD = "cold"   # å†å²å½’æ¡£


@dataclass
class StructuredSummary:
    """
    ç»“æ„åŒ–æ‘˜è¦ - è®©ç¬¬äºŒå¤§è„‘è¶Šæ¥è¶Šèªæ˜çš„æ ¸å¿ƒæ•°æ®ç±»
    
    9 ä¸ªå­—æ®µè®¾è®¡ï¼š
    - core_output: æœ¬æ¬¡æ ¸å¿ƒäº§å‡º
    - tech_points: æŠ€æœ¯è¦ç‚¹
    - code_pattern: ä»£ç æ¨¡å¼
    - decision_context: å†³ç­–ä¸Šä¸‹æ–‡
    - pitfall_record: é¿å‘è®°å½•
    - applicable_scene: é€‚ç”¨åœºæ™¯
    - search_keywords: æœç´¢å…³é”®è¯
    - projectå…³è”: é¡¹ç›®å…³è”
    - confidence: ç½®ä¿¡åº¦
    """
    core_output: str = ""
    tech_points: List[str] = None
    code_pattern: str = ""
    decision_context: str = ""
    pitfall_record: str = ""
    applicable_scene: str = ""
    search_keywords: List[str] = None
    projectå…³è”: str = ""
    confidence: str = "medium"
    
    def __post_init__(self):
        if self.tech_points is None:
            self.tech_points = []
        if self.search_keywords is None:
            self.search_keywords = []
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'StructuredSummary':
        return cls(
            core_output=data.get("æœ¬æ¬¡æ ¸å¿ƒäº§å‡º", ""),
            tech_points=data.get("æŠ€æœ¯è¦ç‚¹", []),
            code_pattern=data.get("ä»£ç æ¨¡å¼", ""),
            decision_context=data.get("å†³ç­–ä¸Šä¸‹æ–‡", ""),
            pitfall_record=data.get("é¿å‘è®°å½•", ""),
            applicable_scene=data.get("é€‚ç”¨åœºæ™¯", ""),
            search_keywords=data.get("æœç´¢å…³é”®è¯", []),
            projectå…³è”=data.get("é¡¹ç›®å…³è”", ""),
            confidence=data.get("ç½®ä¿¡åº¦", "medium")
        )
    
    def to_searchable_text(self) -> str:
        parts = [
            self.core_output,
            " ".join(self.tech_points),
            self.code_pattern,
            self.decision_context,
            self.pitfall_record,
            self.applicable_scene,
            " ".join(self.search_keywords),
            self.projectå…³è”,
        ]
        return " ".join(p for p in parts if p)
    
    def to_tags(self) -> str:
        return ",".join(self.search_keywords)


@dataclass
class ContextEntry:
    """ä¸Šä¸‹æ–‡æ¡ç›®"""
    content: str
    source: str
    relevance_score: float
    injected_at: str
    usage_count: int = 0
    
    def to_dict(self) -> Dict:
        return asdict(self)


class SummaryParser:
    """
    æ‘˜è¦è§£æå™¨
    
    æ”¯æŒï¼š
    - JSON æ ¼å¼ç»“æ„åŒ–æ‘˜è¦ï¼ˆæ–°æ ‡å‡†ï¼‰
    - æ—§æ ¼å¼å…¼å®¹ï¼ˆ---SUMMARY---ï¼‰
    """
    
    JSON_PATTERN = re.compile(
        r'```json\s*\n([\s\S]*?)\n```',
        re.DOTALL
    )
    
    LEGACY_PATTERNS = [
        re.compile(r'## ğŸ“‹ æ€»ç»“[^\n]*\n([\s\S]*?)(?=\n\n|$)', re.DOTALL),
        re.compile(r'---SUMMARY---\s*(.+?)\s*---END---', re.DOTALL | re.IGNORECASE),
    ]
    
    @classmethod
    def parse(cls, response: str) -> tuple:
        """è§£æ LLM å›å¤ï¼Œæå–æ‘˜è¦"""
        summary = None
        
        # ä¼˜å…ˆè§£æ JSON æ ¼å¼
        json_match = cls.JSON_PATTERN.search(response)
        if json_match:
            json_str = json_match.group(1).strip()
            try:
                data = json.loads(json_str)
                summary = StructuredSummary.from_dict(data)
                response = cls.JSON_PATTERN.sub('', response).strip()
            except (json.JSONDecodeError, AttributeError):
                pass
        
        # å…¼å®¹æ—§æ ¼å¼
        if summary is None:
            for pattern in cls.LEGACY_PATTERNS:
                match = pattern.search(response)
                if match:
                    summary_text = match.group(1).strip()
                    summary = StructuredSummary(
                        core_output=summary_text,
                        confidence="low"
                    )
                    response = pattern.sub('', response).strip()
                    break
        
        return response, summary
    
    @classmethod
    def create_summary_prompt(cls) -> str:
        """ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦æç¤ºè¯"""
        return """
## ğŸ§  çŸ¥è¯†æ²‰æ·€ï¼ˆæ¯æ¬¡å›å¤å¿…é¡»ï¼‰

è¯·ç”¨ JSON æ ¼å¼æ€»ç»“æœ¬æ¬¡å¯¹è¯è¦ç‚¹ï¼š

```json
{
  "æœ¬æ¬¡æ ¸å¿ƒäº§å‡º": "ä¸€å¥è¯è¯´æ˜è§£å†³äº†ä»€ä¹ˆé—®é¢˜",
  "æŠ€æœ¯è¦ç‚¹": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2"],
  "ä»£ç æ¨¡å¼": "æå–çš„å¯å¤ç”¨ä»£ç ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰",
  "å†³ç­–ä¸Šä¸‹æ–‡": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ–¹æ¡ˆ",
  "é¿å‘è®°å½•": "åº”é¿å…çš„é”™è¯¯/å¼¯è·¯",
  "é€‚ç”¨åœºæ™¯": "è¿™ä¸ªæ–¹æ¡ˆé€‚ç”¨çš„åœºæ™¯",
  "æœç´¢å…³é”®è¯": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "é¡¹ç›®å…³è”": "æ‰€å±é¡¹ç›®ï¼ˆå¯é€‰ï¼‰",
  "ç½®ä¿¡åº¦": "high/medium/low"
}
```

**è¦æ±‚**ï¼š
- æ¯ä¸ªå­—æ®µéƒ½è¦æ€è€ƒåå¡«å†™
- é¿å…æ³›æ³›è€Œè°ˆï¼Œè¦å…·ä½“å¯æ“ä½œ
- é‡ç‚¹çªå‡º"æœªæ¥èƒ½ç”¨åˆ°"çš„ä¿¡æ¯
"""


class ContextEnginePlugin(NexusPlugin):
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡å¼•æ“æ’ä»¶
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ‘˜è¦ç”Ÿæˆä¸å­˜å‚¨
    2. ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆè§¦å‘è¯ã€å¼•ç”¨ï¼‰
    3. å…³é”®è¯è‡ªåŠ¨æ³¨å…¥
    4. ä¼šè¯æ¢å¤
    """
    
    def __init__(self):
        super().__init__()
        self.metadata = PluginMetadata(
            name="context_engine",
            version="3.1.0",
            description="Smart context engine - summaries, injection, keywords",
            dependencies=["nexus_core", "session_manager"],
            hot_reloadable=True,
        )
        self._nexus_core = None
        self._parser = SummaryParser()
        self._trigger_patterns = None
        
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """åˆå§‹åŒ–"""
        try:
            # è·å– nexus_core
            from ..core.plugin_system import get_plugin_registry
            registry = get_plugin_registry()
            self._nexus_core = registry.get("nexus_core")
            
            # ç¼–è¯‘è§¦å‘è¯æ¨¡å¼
            self._trigger_patterns = [
                re.compile(p, re.IGNORECASE)
                for p in [
                    r'è¿˜è®°å¾—(.+?)[å—?ï¼Ÿ]',
                    r'ä¸Šæ¬¡.*æåˆ°(.+)',
                    r'ä¹‹å‰.*è¯´è¿‡(.+)',
                    r'ä¹‹å‰.*è®¨è®º(.+)',
                    r'ä¹‹å‰.*å†³å®š(.+)',
                ]
            ]
            
            return True
        except Exception as e:
            print(f"ContextEngine init failed: {e}")
            return False
    
    async def start(self) -> bool:
        """å¯åŠ¨"""
        return True
    
    async def stop(self) -> bool:
        """åœæ­¢"""
        return True
    
    # ===================== æ‘˜è¦åŠŸèƒ½ =====================
    
    def parse_summary(self, response: str) -> tuple:
        """è§£ææ‘˜è¦"""
        return self._parser.parse(response)
    
    def store_summary(self, conversation_id: str, response: str, 
                      user_query: str = "") -> Dict[str, Any]:
        """
        å­˜å‚¨æ‘˜è¦åˆ°å‘é‡åº“
        
        Args:
            conversation_id: å¯¹è¯ ID
            response: LLM å›å¤
            user_query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            å­˜å‚¨ç»“æœ
        """
        if not self._nexus_core:
            return {"error": "NexusCore not available"}
        
        reply, summary = self.parse_summary(response)
        
        results = {
            "conversation_id": conversation_id,
            "stored_count": 0,
            "has_summary": summary is not None,
        }
        
        try:
            # 1. å­˜å‚¨åŸæ–‡
            self._nexus_core.add_document(
                content=reply,
                title=f"å¯¹è¯ {conversation_id} - åŸæ–‡",
                tags=f"type:content,source:{conversation_id}"
            )
            results["stored_count"] += 1
            
            # 2. å­˜å‚¨æ‘˜è¦
            if summary:
                if isinstance(summary, StructuredSummary):
                    # ç»“æ„åŒ–æ‘˜è¦
                    searchable = summary.to_searchable_text()
                    tags = f"type:structured_summary,confidence:{summary.confidence}"
                    if summary.search_keywords:
                        tags += "," + ",".join(summary.search_keywords)
                    
                    self._nexus_core.add_document(
                        content=searchable,
                        title=f"å¯¹è¯ {conversation_id} - æ‘˜è¦",
                        tags=tags
                    )
                    results["stored_count"] += 1
                    
                    # å…ƒæ•°æ®
                    self._nexus_core.add_document(
                        content=json.dumps(summary.to_dict(), ensure_ascii=False),
                        title=f"å¯¹è¯ {conversation_id} - å…ƒæ•°æ®",
                        tags=f"type:metadata,source:{conversation_id}"
                    )
                    results["stored_count"] += 1
                    
                    results["summary_data"] = summary.to_dict()
                else:
                    # æ—§æ ¼å¼
                    self._nexus_core.add_document(
                        content=summary.core_output,
                        title=f"å¯¹è¯ {conversation_id} - æ‘˜è¦",
                        tags=f"type:summary,source:{conversation_id}"
                    )
                    results["stored_count"] += 1
                    results["summary_data"] = {"core_output": summary.core_output}
                    
        except Exception as e:
            results["error"] = str(e)
        
        return results
    
    # ===================== è§¦å‘è¯æ£€æµ‹ =====================
    
    def detect_trigger(self, user_message: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æµ‹è§¦å‘è¯
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            è§¦å‘ç»“æœæˆ– None
        """
        if not self._trigger_patterns:
            return None
        
        for pattern in self._trigger_patterns:
            match = pattern.search(user_message)
            if match:
                return {
                    "triggered": True,
                    "pattern": match.group(0),
                    "query": user_message[match.end():].strip().rstrip("å—?ï¼Ÿ") or user_message[:match.start()].strip(),
                    "original_message": user_message
                }
        
        return None
    
    def resolve_reference(self, query: str, limit: int = 3) -> List[Dict]:
        """
        è§£æå¼•ç”¨ï¼Œæ£€ç´¢ç›¸å…³å†å²
        
        Args:
            query: æŸ¥è¯¢è¯
            limit: è¿”å›æ•°é‡
            
        Returns:
            ç›¸å…³å†å²åˆ—è¡¨
        """
        if not self._nexus_core:
            return []
        
        try:
            from ..plugins.nexus_core import RecallResult
            results = self._nexus_core.search_recall(query, n=limit)
            
            return [
                {
                    "content": r.content,
                    "source": r.source,
                    "relevance": r.relevance,
                    "metadata": r.metadata,
                }
                for r in results
            ]
        except Exception:
            return []
    
    # ===================== å…³é”®è¯æ³¨å…¥ =====================
    
    def extract_keywords(self, text: str, max_count: int = 5) -> List[str]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬
            max_count: æœ€å¤§æ•°é‡
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•åˆ†è¯
        words = re.findall(r'\b\w+\b', text.lower())
        
        # åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 
                      'å’Œ', 'ä¸', 'æˆ–', 'å°±', 'éƒ½', 'ä¹Ÿ', 'ä¼š', 'å¯ä»¥', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•'}
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        
        # å»é‡è¿”å›
        return list(dict.fromkeys(keywords))[:max_count]
    
    def inject_keywords(self, conversation: str, limit: int = 3) -> List[Dict]:
        """
        å…³é”®è¯è‡ªåŠ¨æ³¨å…¥
        
        Args:
            conversation: å¯¹è¯å†…å®¹
            limit: æ¯ä¸ªå…³é”®è¯è¿”å›æ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        keywords = self.extract_keywords(conversation, 5)
        results = []
        
        for keyword in keywords:
            related = self.resolve_reference(keyword, limit)
            for r in related:
                if r not in results:
                    results.append(r)
        
        return results[:10]  # æœ€å¤šè¿”å› 10 æ¡
    
    # ===================== ä¼šè¯æ¢å¤ =====================
    
    def resume_session(self, session_id: str, topic: str = "", 
                       limit: int = 5) -> List[Dict]:
        """
        ä¼šè¯æ¢å¤ï¼Œæ£€ç´¢ç›¸å…³å†å²
        
        Args:
            session_id: ä¼šè¯ ID
            topic: è¯é¢˜
            limit: è¿”å›æ•°é‡
            
        Returns:
            ç›¸å…³å†å²åˆ—è¡¨
        """
        return self.resolve_reference(topic or session_id, limit)
    
    # ===================== ç”Ÿæˆæç¤ºè¯ =====================
    
    def generate_context_prompt(self, 
                               references: List[Dict],
                               system_prompt: str = "") -> str:
        """
        ç”Ÿæˆä¸Šä¸‹æ–‡æç¤ºè¯
        
        Args:
            references: å‚è€ƒåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            å®Œæ•´æç¤ºè¯
        """
        if not references:
            return system_prompt
        
        context_parts = [
            system_prompt,
            "",
            "## ç›¸å…³å†å²ä¸Šä¸‹æ–‡",
            ""
        ]
        
        for i, ref in enumerate(references, 1):
            context_parts.append(f"ã€å†å² {i}ã€‘({ref.get('source', 'æœªçŸ¥')})")
            context_parts.append(ref.get('content', '')[:500])
            context_parts.append("")
        
        return "\n".join(context_parts)


# ===================== å‘åå…¼å®¹ =====================

# ä¾¿æ·å‡½æ•°
_parser = SummaryParser()


def parse_summary(response: str) -> tuple:
    """è§£ææ‘˜è¦ï¼ˆå…¼å®¹æ—§ APIï¼‰"""
    return _parser.parse(response)


def create_summary_prompt() -> str:
    """ç”Ÿæˆæ‘˜è¦æç¤ºè¯"""
    return _parser.create_summary_prompt()


# CLI å…¥å£
if __name__ == "__main__":
    import sys
    
    print("=" * 60)
    print("ğŸ§  Context Engine - æ™ºèƒ½ä¸Šä¸‹æ–‡å¼•æ“")
    print("=" * 60)
    
    # æµ‹è¯•è§£æ
    test_response = """
è¿™æ˜¯æµ‹è¯•å›å¤ã€‚

```json
{
  "æœ¬æ¬¡æ ¸å¿ƒäº§å‡º": "æµ‹è¯•ç»“æ„åŒ–æ‘˜è¦åŠŸèƒ½",
  "æŠ€æœ¯è¦ç‚¹": ["æµ‹è¯•", "è§£æ"],
  "ä»£ç æ¨¡å¼": "print('hello')",
  "å†³ç­–ä¸Šä¸‹æ–‡": "æµ‹è¯•ç›®çš„",
  "é¿å‘è®°å½•": "æ— ",
  "é€‚ç”¨åœºæ™¯": "å•å…ƒæµ‹è¯•",
  "æœç´¢å…³é”®è¯": ["æµ‹è¯•", "ä¸Šä¸‹æ–‡"],
  "é¡¹ç›®å…³è”": "Context Engine",
  "ç½®ä¿¡åº¦": "high"
}
```
"""
    
    reply, summary = parse_summary(test_response)
    
    print("\nâœ… æµ‹è¯•è§£æ:")
    print(f"  åŸæ–‡: {reply[:50]}...")
    if summary:
        print(f"  æ ¸å¿ƒäº§å‡º: {summary.core_output}")
        print(f"  æŠ€æœ¯è¦ç‚¹: {summary.tech_points}")
        print(f"  ç½®ä¿¡åº¦: {summary.confidence}")
    
    print("\nâœ… Context Engine æ­£å¸¸å·¥ä½œ")
